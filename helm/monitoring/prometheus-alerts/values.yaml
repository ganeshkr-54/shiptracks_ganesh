prefix: prometheus-alerts
namespace: monitoring
labels:
  app: kube-prometheus-stack
  release: prometheus-stack

alerts:
  - kafka
  - certificates
  - pipelines-main
  - pipelines-other
  - environments-status
  - quartz-jobs
  - ingress
  - blackbox

# Blackbox exporter
blackbox:
  enabled: true
  rules:
    - alert: BlackboxProbeFailed
      expr: probe_success == 0
      for: 1m
      annotations:
        summary: Blackbox probe failed (instance {{ $labels.instance }})
        description: "Probe failed\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
      labels:
        responsible: DevOps Team
        severity: critical
        route: blackbox
    - alert: BlackboxProbeHttpFailure
      expr: probe_http_status_code <= 199 OR probe_http_status_code > 400
      for: 1m
      annotations:
        summary: Blackbox probe HTTP failure (instance {{ $labels.instance }})
        description: "HTTP status code is not 200-399 [400 for graphql]\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
      labels:
        responsible: DevOps Team
        severity: critical
        route: blackbox

# Ingress nginx
# ingress:
#   enabled: false
#   rules:
#     - alert: NginxHighHttp4xxErrorRate
#       expr: >-
#         sum(rate(nginx_ingress_controller_requests{status=~"4.."}[1m])) / sum(rate(nginx_ingress_controller_requests[1m])) * 100 > 0.1
#       for: 5m
#       annotations:
#         summary: Nginx high HTTP 4xx error rate (instance {{ $labels.instance }})
#         description: >-
#           Too many HTTP requests to (instance {{ $labels.instance }}) with status 4xx (> 5%).
#         dashboard: "{{ $externalURL }}/../d/ingress/helm-ingress"
#       labels:
#         responsible: DevOps Team
#         severity: critical
#         route: ingress
#     - alert: NginxHighHttp5xxErrorRate
#       expr: >-
#         sum(rate(nginx_ingress_controller_requests{status=~"^5.."}[1m])) / sum(rate(nginx_ingress_controller_requests[1m])) * 100 > 0.1
#       for: 5m
#       annotations:
#         summary: Nginx high HTTP 5xx error rate (instance {{ $labels.instance }})
#         description: >-
#           Too many HTTP requests to (instance {{ $labels.instance }}) with status 5xx (> 5%).
#         dashboard: "{{ $externalURL }}/../d/ingress/helm-ingress"
#       labels:
#         responsible: DevOps Team
#         severity: critical
#         route: ingress
#     - alert: NginxLatencyHigh
#       expr: >-
#         histogram_quantile(0.99, sum(rate(nginx_ingress_controller_request_duration_seconds_bucket[30m])) by (host, node)) > 3
#       for: 5m
#       annotations:
#         description: >-
#           Nginx latency high (instance {{ $labels.instance }}), p99 latency is higher than 3 seconds
#         dashboard: "{{ $externalURL }}/../d/ingress/helm-ingress"
#       labels:
#         responsible: DevOps Team
#         severity: warning
#         route: ingress
#     - alert: NginxHighHttp4xxErrorService
#       expr: >-
#         rate((nginx_ingress_controller_requests{status=~"2.."}[5m])) > 0.1
#       for: 5m
#       annotations:
#         summary: HTTP 4xx error service (instance {{ $labels.ingress }})
#         description: >-
#           HTTP 5xx error (service {{ $labels.ingress }}) with status 4xx.
#         dashboard: "{{ $externalURL }}/../d/ingress/helm-ingress"
#       labels:
#         responsible: DevOps Team
#         severity: critical
#         route: ingress
#     - alert: NginxHighHttp5xxErrorService
#       expr: >-
#         rate((nginx_ingress_controller_requests{status=~"2.."}[5m])) > 0.1
#       for: 5m
#       annotations:
#         summary: HTTP 5xx error service (service {{ $labels.ingress }})
#         description: >-
#          HTTP 5xx error (service {{ $labels.ingress }}) with status 5xx.
#         dashboard: "{{ $externalURL }}/../d/ingress/helm-ingress"
#       labels:
#         responsible: DevOps Team
#         severity: critical
#         route: ingress

# Environments Status
environments-status:
  enabled: false
  rules:
    - alert: ApplicationDown
      expr: >-
        probe_success{environment=~"Production|Preview"} != 1
      for: 1m
      annotations:
        description: >-
          CRITICAL: {{ $labels.environment }} {{ $labels.service }} is DOWN!
          URL to check: {{ $labels.instance }}
        dashboard: "{{ $externalURL }}/../d/helm-environments-status"
      labels:
        responsible: DevOps Team
        severity: critical
        route: critical
    - alert: HighUsage
      expr: >-
        avg(irate(node_cpu_seconds_total{mode="idle"}[1m]) * 100) < 50
      for: 1m
      annotations:
        summary: "High usage on {{ $labels.instance }}"
        description: "{{ $labels.instance }} has a average CPU idle (current value: {{ $value }}s)"
        dashboard: "{{ $externalURL }}/../d/helm-pods-resources"
      labels:
        responsible: DevOps Team
        severity: critical
        route: critical
    - alert: POD_MEMORY_HIGH_UTILIZATION
      expr: >-
        (sum(rate(container_memory_usage_bytes[5m]))by(pod_name)*100) >80
      for: 1m
      annotations:
        summary: "HIGH Memory USAGE WARNING for{{$labels.pod}}"
        description: "pod {{$labels.pod}} is using high memory"
        dashboard: "{{ $externalURL }}/../d/helm-pods-resources"
      labels:
        responsible: DevOps Team
        severity: critical
        route: critical
# Certificates
certificates:
  enabled: false
  rules:
    - alert: CertificateExpirationKubernetes
      expr: >-
        ((x509_cert_not_after - time()) / 86400) <= 14
      for: 5m
      annotations:
        description: >-
          Certificate for "{{ $labels.subject_CN }}" ({{if $labels.secret_name }}secret: "{{ $labels.secret_namespace }}/{{ $labels.secret_name }}"{{else}}location "{{ $labels.filepath }}"{{end}}) is about to expire: {{ printf "%.0f" $value }} days left.
        dashboard: "{{ $externalURL }}/../d/helm-certificates"
      labels:
        responsible: DevOps Team
        severity: warning
        route: certificates
    - alert: CertificateExpirationExternal
      expr: >-
        ((ssl_cert_expiry_time - time()) / 86400) <= 14
      for: 5m
      annotations:
        description: >-
          Certificate for "{{ $labels.connect_host }}" ("{{ $labels.subject }}") is about to expire: {{ printf "%.0f" $value }} days left.
        dashboard: "{{ $externalURL }}/../d/helm-certificates"
      labels:
        responsible: DevOps Team
        severity: warning
        route: certificates

# Kafka
kafka:
  enabled: false
  rules:
    - alert: KafkaConsumerGroupMaxLagAboveThreshold
      expr: >-
        max_over_time(kafka_consumergroup_group_max_lag_seconds[20m]) > 2 * 3600
      for: 15m
      annotations:
        description: >-
          Kafka "{{ $labels.cluster_name }}" consumer group "{{ $labels.group }}" lag is above 2 hours: {{ printf "%.0f" $value }} seconds.
        dashboard: "{{ $externalURL }}/../d/helm-kafka"
      labels:
        responsible: Stargazers Team
        severity: warning
        route: kafka

# Pipelines (main)
pipelines-main:
  enabled: false
  rules:
    # Vessels
    - alert: VesselsUpdatedBelowThreshold7d
      expr: >-
        (min_over_time(vessels_updated{interval="7d"}[20m]) or on() label_replace(vector(0), "value", "empty", "value", "")) < 50000
      for: 15m
      annotations:
        description: >-
          Vessels updated in the last {{ $labels.interval }} are below 50000: {{ printf "%.0f" $value }}.
        dashboard: "{{ $externalURL }}/../d/helm-pipelines"
      labels:
        responsible: Stargazers Team
        severity: warning
        route: pipelines
    - alert: VesselsUpdatedBelowThreshold1d
      expr: >-
        (min_over_time(vessels_updated{interval="1d"}[20m]) or on() label_replace(vector(0), "value", "empty", "value", "")) < 30000
      for: 15m
      annotations:
        description: >-
          Vessels updated in the last {{ $labels.interval }} are below 30000: {{ printf "%.0f" $value }}.
        dashboard: "{{ $externalURL }}/../d/helm-pipelines"
      labels:
        responsible: Stargazers Team
        severity: warning
        route: pipelines
    - alert: VesselsUpdatedBelowThreshold5m
      expr: >-
        (min_over_time(vessels_updated{interval="5m"}[20m]) or on() label_replace(vector(0), "value", "empty", "value", "")) < 15000
      for: 15m
      annotations:
        description: >-
          Vessels updated in the last {{ $labels.interval }} are below 15000: {{ printf "%.0f" $value }}.
        dashboard: "{{ $externalURL }}/../d/helm-pipelines"
      labels:
        responsible: Stargazers Team
        severity: warning
        route: pipelines
    # AIS
    # - alert: AISUpdatedBelowThreshold
    #   expr: >-
    #     (min_over_time(ais_updated[20m]) or on() label_replace(vector(0), "value", "empty", "value", "")) <= 0
    #   for: 5m
    #   annotations:
    #     description: >-
    #       AIS {{ $labels.source }} updated in the last {{ $labels.interval }} are zero: {{ printf "%.0f" $value }}.
    #     dashboard: "{{ $externalURL }}/../d/helm-pipelines"
    #   labels:
    #     responsible: Stargazers Team
    #     severity: warning
    #     route: pipelines
    # - alert: AISArchiveUpdatedBelowThreshold
    #   expr: >-
    #     (min_over_time(ais_archive_updated[20m]) or on() label_replace(vector(0), "value", "empty", "value", "")) <= 0
    #   for: 5m
    #   annotations:
    #     description: >-
    #       AIS Archive {{ $labels.source }} updated in the last {{ $labels.interval }} are zero: {{ printf "%.0f" $value }}.
    #     dashboard: "{{ $externalURL }}/../d/helm-pipelines"
    #   labels:
    #     responsible: Stargazers Team
    #     severity: warning
    #     route: pipelines
    # - alert: AISType5ArchiveUpdatedBelowThreshold
    #   expr: >-
    #     (min_over_time(ais_type5_archive_updated[20m]) or on() label_replace(vector(0), "value", "empty", "value", "")) <= 0
    #   for: 5m
    #   annotations:
    #     description: >-
    #       AIS Type 5 Archive {{ $labels.source }} updated in the last {{ $labels.interval }} are zero: {{ printf "%.0f" $value }}.
    #     dashboard: "{{ $externalURL }}/../d/helm-pipelines"
    #   labels:
    #     responsible: Stargazers Team
    #     severity: warning
    #     route: pipelines

# Pipelines (other)
pipelines-other:
  enabled: false
  rules:
    # Events
    - alert: EventsUpdatedBelowThreshold
      expr: >-
        (min_over_time(events_updated[20m]) or on() label_replace(vector(0), "value", "empty", "value", "")) <= 0
      for: 15m
      annotations:
        description: >-
          Events "{{ $labels.event }}" updated in the last {{ $labels.interval }} are zero: {{ printf "%.0f" $value }}.
        dashboard: "{{ $externalURL }}/../d/helm-pipelines"
      labels:
        responsible: Stargazers Team
        severity: warning
        route: pipelines
    # Events count
    - alert: ZoneEventsCountBelowThreshold
      expr: >-
        (min_over_time(zone_events_count[20m]) or on() label_replace(vector(0), "value", "empty", "value", "")) <= 0
      for: 15m
      annotations:
        description: >-
          Zone "{{ $labels.zone }}" events count is zero: {{ printf "%.0f" $value }}.
        dashboard: "{{ $externalURL }}/../d/helm-pipelines"
      labels:
        responsible: Stargazers Team
        severity: warning
        route: pipelines
    - alert: AisEventsCountBelowThreshold
      expr: >-
        (min_over_time(ais_events_count[20m]) or on() label_replace(vector(0), "value", "empty", "value", "")) <= 0
      for: 15m
      annotations:
        description: >-
          AIS "{{ $labels.event }}" events count is zero: {{ printf "%.0f" $value }}.
        dashboard: "{{ $externalURL }}/../d/helm-pipelines"
      labels:
        responsible: Stargazers Team
        severity: warning
        route: pipelines
    # Aggregations
    - alert: AggregationsUpdatedBelowThreshold
      expr: >-
        (min_over_time(aggregations_updated[20m]) or on() label_replace(vector(0), "value", "empty", "value", "")) <= 0
      for: 15m
      annotations:
        description: >-
          Aggregations "{{ $labels.aggregation }}" updated in the last {{ $labels.interval }} are zero: {{ printf "%.0f" $value }}.
        dashboard: "{{ $externalURL }}/../d/helm-pipelines"
      labels:
        responsible: Stargazers Team
        severity: warning
        route: pipelines

# Quartz Jobs
quartz-jobs:
  enabled: false
  rules:
    - alert: QuartzJobsNotUpdatedGreaterThanZero
      expr: >-
        min_over_time(quartz_jobs_not_updated[20m]) > 0
      for: 30m
      annotations:
        description: >-
          Total number of enabled and not updated "quartz_job" is greater than zero: {{ printf "%.0f" $value }}.
        dashboard: "{{ $externalURL }}/../d/quartz-jobs"
      labels:
        responsible: Deepthi Tammana (stammana@oceaneering.com)
        severity: warning
        route: main
